{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                orig|label|                Text|\n",
      "+--------------------+-----+--------------------+\n",
      "|bunu da gördük uz...|    0|bunu da gördük uz...|\n",
      "|filmi begenmeyenl...|    0|filmi begenmeyenl...|\n",
      "|günde 1 film izle...|    0|günde 1 film izle...|\n",
      "|afisine bakip ald...|    0|afisine bakip ald...|\n",
      "|sadece insanlarin...|    0|sadece insanlarin...|\n",
      "|ucuz bir aksiyon ...|    0|ucuz bir aksiyon ...|\n",
      "|olmamis diyor pua...|    0|olmamis diyor pua...|\n",
      "|kesinlikle çok kö...|    0|kesinlikle cok kö...|\n",
      "|ben nasil bir fil...|    0|ben nasil bir fil...|\n",
      "|bu yüzler fazla t...|    0|bu yüzler fazla t...|\n",
      "|. çok ilginç yaa ...|    0| cok ilginc yaa k...|\n",
      "|bence bu film hiç...|    0|bence bu film hic...|\n",
      "|valla ben begenme...|    0|valla ben begenme...|\n",
      "|gönül ister milyo...|    0|gönül ister milyo...|\n",
      "|dün aksam bu film...|    0|dün aksam bu film...|\n",
      "|cok kötü olmus......|    0|cok kötü olmus am...|\n",
      "|igrenç yaaa.acaba...|    0|igrenc yaaaacaba ...|\n",
      "|saçma sapan bir f...|    0|sacma sapan bir f...|\n",
      "|çok güzel komedi ...|    0|cok güzel komedi ...|\n",
      "|beklentimi yüksek...|    0|beklentimi yüksek...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy :  0.8406988694758484\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(3000,[0,1,2,3,5,...|    1|[7.06860535461208...|[0.35343026773060...|       1.0|\n",
      "|(3000,[0,1,2,3,6,...|    0|[10.5627996696098...|[0.52813998348049...|       0.0|\n",
      "|(3000,[0,1,2,3,7,...|    1|[9.95553054222452...|[0.49777652711122...|       1.0|\n",
      "|(3000,[0,1,2,3,7,...|    0|[10.6009334862416...|[0.53004667431208...|       0.0|\n",
      "|(3000,[0,1,2,3,9,...|    1|[7.44523806152681...|[0.37226190307634...|       1.0|\n",
      "|(3000,[0,1,2,3,19...|    0|[10.5440960313040...|[0.52720480156520...|       0.0|\n",
      "|(3000,[0,1,2,4,16...|    0|[9.97677020475757...|[0.49883851023787...|       1.0|\n",
      "|(3000,[0,1,2,5,6,...|    1|[9.32431063710588...|[0.46621553185529...|       1.0|\n",
      "|(3000,[0,1,2,5,6,...|    1|[9.57065423064293...|[0.47853271153214...|       1.0|\n",
      "|(3000,[0,1,2,5,6,...|    1|[8.65005750805893...|[0.43250287540294...|       1.0|\n",
      "|(3000,[0,1,2,5,7,...|    1|[7.85583612078950...|[0.39279180603947...|       1.0|\n",
      "|(3000,[0,1,2,5,9,...|    1|[10.6857096468693...|[0.53428548234346...|       0.0|\n",
      "|(3000,[0,1,2,5,11...|    0|[9.06059333930009...|[0.45302966696500...|       1.0|\n",
      "|(3000,[0,1,2,5,24...|    1|[9.58791977947735...|[0.47939598897386...|       1.0|\n",
      "|(3000,[0,1,2,6,14...|    0|[10.5586640921605...|[0.52793320460802...|       0.0|\n",
      "|(3000,[0,1,2,6,73...|    1|[9.26042916917085...|[0.46302145845854...|       1.0|\n",
      "|(3000,[0,1,2,6,12...|    1|[10.0797156845707...|[0.50398578422853...|       0.0|\n",
      "|(3000,[0,1,2,7,8,...|    1|[8.22263202588288...|[0.41113160129414...|       1.0|\n",
      "|(3000,[0,1,2,7,16...|    0|[8.64009135328415...|[0.43200456766420...|       1.0|\n",
      "|(3000,[0,1,2,8,11...|    1|[8.20072098910567...|[0.41003604945528...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit,ParamGridBuilder\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def removePunctuation(text):\n",
    "    text = text.replace('.','')\n",
    "    text = text.replace(',','')\n",
    "    text = text.replace(';','')\n",
    "    text = text.replace(':','')\n",
    "    text = text.replace('ç','c')\n",
    "    text = text.replace('ş','s')\n",
    "    return text\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "textDF = spark.read.option('delimiter','\\t').option('inferSchema','true').csv('datasets/movie_turkish_train.txt')\n",
    "textDF = textDF.withColumnRenamed('_c0','orig')\n",
    "textDF = textDF.withColumnRenamed('_c1','label')\n",
    "\n",
    "myUDF = UserDefinedFunction(removePunctuation,StringType())\n",
    "\n",
    "textDF = textDF.withColumn('Text',myUDF('orig'))\n",
    "\n",
    "\n",
    "textDF.show()\n",
    "tokenizer = Tokenizer(inputCol='Text',outputCol='tokenized')\n",
    "textDF = tokenizer.transform(textDF)\n",
    "\n",
    "trStopWords = StopWordsRemover.loadDefaultStopWords('turkish')\n",
    "sRemover = StopWordsRemover(inputCol='tokenized',outputCol='removed',stopWords=trStopWords)\n",
    "textDF = sRemover.transform(textDF)\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol='removed',outputCol='features',vocabSize=3000)\n",
    "textDF = vectorizer.fit(textDF).transform(textDF)\n",
    "\n",
    "textDF = textDF.select('features','label')\n",
    "trainDF, testDF = textDF.randomSplit([0.75,0.25],seed=123) \n",
    "\n",
    "rfClassifier= RandomForestClassifier()\n",
    "model = rfClassifier.fit(trainDF)\n",
    "\n",
    "resultDF = model.transform(testDF)\n",
    "\n",
    "eva = BinaryClassificationEvaluator()\n",
    "successRate = eva.evaluate(resultDF)\n",
    "print(\"Accuracy : \",successRate)\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df.select(\n",
    "    [F.regexp_replace(col, r',|\\.|&|\\\\|\\||-|_', '').alias(col) for col in df.columns]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
